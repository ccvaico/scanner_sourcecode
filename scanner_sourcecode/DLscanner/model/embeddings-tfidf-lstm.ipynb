{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5"
   },
   "outputs": [],
   "source": [
    "import keras\n",
    "print(keras.__version__)\n",
    "import numpy as np \n",
    "import pandas as pd \n",
    "import tensorflow as tf\n",
    "print(tf.__version__)\n",
    "import zipfile\n",
    "from matplotlib import pyplot as plt\n",
    "from tqdm import tqdm\n",
    "%matplotlib inline\n",
    "import sys\n",
    "import datetime\n",
    "import time\n",
    "import nltk, re, string\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import SnowballStemmer\n",
    "from keras.models import Model\n",
    "from keras.layers import Input, Dense, Embedding, concatenate\n",
    "from keras.layers import Bidirectional, GlobalAveragePooling1D, GlobalMaxPooling1D, Conv1D\n",
    "from keras.layers import LSTM, CuDNNGRU, Add, BatchNormalization, Activation, CuDNNLSTM, Dropout\n",
    "from keras.layers import *\n",
    "from keras.models import *\n",
    "from keras.preprocessing import text, sequence\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint, ReduceLROnPlateau\n",
    "import gc\n",
    "from sklearn import metrics\n",
    "# XGboost related\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "\n",
    "from scipy.sparse import csr_matrix, hstack\n",
    "\n",
    "from collections import Counter\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "color = sns.color_palette()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
    "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a"
   },
   "outputs": [],
   "source": [
    "train_df = pd.read_csv('train.csv',sep='\\t')\n",
    "#train_df[\"question_text\"] = train_df[\"question_text\"].map(lambda x: clean_text(x))\n",
    "\n",
    "test_df = pd.read_csv('test.csv',sep='\\t')\n",
    "#test_df[\"question_text\"] = test_df[\"question_text\"].map(lambda x: clean_text(x))\n",
    "\n",
    "# train_ques_lens = train_df['question_text'].map(lambda x: len(x.split(' ')))\n",
    "# test_ques_lens = test_df['question_text'].map(lambda x: len(x.split(' ')))\n",
    "# print('Train text max len:', train_ques_lens.max())\n",
    "# print('Test text max len:', test_ques_lens.max())\n",
    "# plt.figure(figsize=(10, 4))\n",
    "# sns.kdeplot(train_ques_lens)\n",
    "# sns.kdeplot(test_ques_lens)\n",
    "# plt.legend(('train', 'test'))\n",
    "# plt.show()\n",
    "# del train_ques_lens; del test_ques_lens\n",
    "# gc.collect()\n",
    "# pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "6b313a09bcacdbe5a135ba9201a713e4e148f2cb"
   },
   "outputs": [],
   "source": [
    "maxlen = 70\n",
    "max_features = 90000  # capsule 95000\n",
    "embed_size = 300"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "5e5893caf09171995d6e87cc740295f6dec95c5c"
   },
   "outputs": [],
   "source": [
    "puncts = [',', '.', '\"', ':', ')', '(', '-', '!', '?', '|', ';', \"'\", '$', '&', '/', '[', ']', '>', '%', '=', '#', '*', '+', '\\\\', '•',  '~', '@', '£', \n",
    " '·', '_', '{', '}', '©', '^', '®', '`',  '<', '→', '°', '€', '™', '›',  '♥', '←', '×', '§', '″', '′', 'Â', '█', '½', 'à', '…', \n",
    " '“', '★', '”', '–', '●', 'â', '►', '−', '¢', '²', '¬', '░', '¶', '↑', '±', '¿', '▾', '═', '¦', '║', '―', '¥', '▓', '—', '‹', '─', \n",
    " '▒', '：', '¼', '⊕', '▼', '▪', '†', '■', '’', '▀', '¨', '▄', '♫', '☆', 'é', '¯', '♦', '¤', '▲', 'è', '¸', '¾', 'Ã', '⋅', '‘', '∞', \n",
    " '∙', '）', '↓', '、', '│', '（', '»', '，', '♪', '╩', '╚', '³', '・', '╦', '╣', '╔', '╗', '▬', '❤', 'ï', 'Ø', '¹', '≤', '‡', '√', ]\n",
    "\n",
    "punct_mapping = {\"‘\": \"'\", \"₹\": \"e\", \"´\": \"'\", \"°\": \"\", \"€\": \"e\", \"™\": \"tm\", \"√\": \" sqrt \", \"×\": \"x\", \"²\": \"2\", \"—\": \"-\", \"–\": \"-\", \"’\": \"'\", \"_\": \"-\", \"`\": \"'\", \n",
    "                 '“': '\"', '”': '\"', '“': '\"', \"£\": \"e\", '∞': 'infinity', 'θ': 'theta', '÷': '/', 'α': 'alpha', '•': '.', 'à': 'a', '−': '-', 'β': 'beta', '∅': '', \n",
    "                 '³': '3', 'π': 'pi', }\n",
    "\n",
    "def clean_text(x):\n",
    "    x = str(x)\n",
    "    for p in punct_mapping:\n",
    "        x = x.replace(p, punct_mapping[p])\n",
    "    for punct in puncts:\n",
    "        x = x.replace(punct, f' {punct} ')\n",
    "    return x\n",
    "\n",
    "def clean_numbers(x):\n",
    "    x = re.sub('[0-9]{5,}', '#####', x)\n",
    "    x = re.sub('[0-9]{4}', '####', x)\n",
    "    x = re.sub('[0-9]{3}', '###', x)\n",
    "    x = re.sub('[0-9]{2}', '##', x)\n",
    "    return x\n",
    "\n",
    "mispell_dict = {\"ain't\": \"is not\", \"aren't\": \"are not\",\"can't\": \"cannot\", \"'cause\": \"because\", \"could've\": \"could have\", \"couldn't\": \"could not\", \n",
    "                \"didn't\": \"did not\",  \"doesn't\": \"does not\", \"don't\": \"do not\", \"hadn't\": \"had not\", \"hasn't\": \"has not\", \"haven't\": \"have not\", \n",
    "                \"he'd\": \"he would\",\"he'll\": \"he will\", \"he's\": \"he is\", \"how'd\": \"how did\", \"how'd'y\": \"how do you\", \"how'll\": \"how will\", \n",
    "                \"how's\": \"how is\",  \"I'd\": \"I would\", \"I'd've\": \"I would have\", \"I'll\": \"I will\", \"I'll've\": \"I will have\",\"I'm\": \"I am\", \n",
    "                \"I've\": \"I have\", \"i'd\": \"i would\", \"i'd've\": \"i would have\", \"i'll\": \"i will\",  \"i'll've\": \"i will have\",\"i'm\": \"i am\", \n",
    "                \"i've\": \"i have\", \"isn't\": \"is not\", \"it'd\": \"it would\", \"it'd've\": \"it would have\", \"it'll\": \"it will\", \"it'll've\": \"it will have\",\n",
    "                \"it's\": \"it is\", \"let's\": \"let us\", \"ma'am\": \"madam\", \"mayn't\": \"may not\", \"might've\": \"might have\",\"mightn't\": \"might not\",\n",
    "                \"mightn't've\": \"might not have\", \"must've\": \"must have\", \"mustn't\": \"must not\", \"mustn't've\": \"must not have\", \"needn't\": \n",
    "                \"need not\", \"needn't've\": \"need not have\",\"o'clock\": \"of the clock\", \"oughtn't\": \"ought not\", \"oughtn't've\": \"ought not have\", \n",
    "                \"shan't\": \"shall not\", \"sha'n't\": \"shall not\", \"shan't've\": \"shall not have\", \"she'd\": \"she would\", \"she'd've\": \"she would have\", \n",
    "                \"she'll\": \"she will\", \"she'll've\": \"she will have\", \"she's\": \"she is\", \"should've\": \"should have\", \"shouldn't\": \"should not\", \n",
    "                \"shouldn't've\": \"should not have\", \"so've\": \"so have\",\"so's\": \"so as\", \"this's\": \"this is\",\"that'd\": \"that would\", \"that'd've\": \"that would have\", \n",
    "                \"that's\": \"that is\", \"there'd\": \"there would\", \"there'd've\": \"there would have\", \"there's\": \"there is\", \"here's\": \"here is\",\"they'd\": \"they would\", \n",
    "                \"they'd've\": \"they would have\", \"they'll\": \"they will\", \"they'll've\": \"they will have\", \"they're\": \"they are\", \"they've\": \"they have\", \"to've\": \"to have\", \n",
    "                \"wasn't\": \"was not\", \"we'd\": \"we would\", \"we'd've\": \"we would have\", \"we'll\": \"we will\", \"we'll've\": \"we will have\", \"we're\": \"we are\", \"we've\": \"we have\", \n",
    "                \"weren't\": \"were not\", \"what'll\": \"what will\", \"what'll've\": \"what will have\", \"what're\": \"what are\",  \"what's\": \"what is\", \"what've\": \"what have\", \n",
    "                \"when's\": \"when is\", \"when've\": \"when have\", \"where'd\": \"where did\", \"where's\": \"where is\", \"where've\": \"where have\", \"who'll\": \"who will\", \n",
    "                \"who'll've\": \"who will have\", \"who's\": \"who is\", \"who've\": \"who have\", \"why's\": \"why is\", \"why've\": \"why have\", \"will've\": \"will have\", \"won't\": \"will not\", \"won't've\": \"will not have\", \"would've\": \"would have\", \"wouldn't\": \"would not\", \"wouldn't've\": \"would not have\", \"y'all\": \"you all\", \"y'all'd\": \"you all would\",\"y'all'd've\": \"you all would have\",\"y'all're\": \"you all are\",\"y'all've\": \"you all have\",\"you'd\": \"you would\", \"you'd've\": \"you would have\", \"you'll\": \"you will\", \"you'll've\": \"you will have\", \"you're\": \"you are\", \"you've\": \"you have\", 'colour': 'color', 'centre': 'center', 'favourite': 'favorite', 'travelling': 'traveling', 'counselling': 'counseling', 'theatre': 'theater', 'cancelled': 'canceled', 'labour': 'labor', 'organisation': 'organization', 'wwii': 'world war 2', 'citicise': 'criticize', 'youtu ': 'youtube ', 'Qoura': 'Quora', 'sallary': 'salary', 'Whta': 'What', 'narcisist': 'narcissist', 'howdo': 'how do', 'whatare': 'what are', 'howcan': 'how can', 'howmuch': 'how much', 'howmany': 'how many', 'whydo': 'why do', 'doI': 'do I', 'theBest': 'the best', 'howdoes': 'how does', 'mastrubation': 'masturbation', 'mastrubate': 'masturbate', \"mastrubating\": 'masturbating', 'pennis': 'penis', 'Etherium': 'Ethereum', 'narcissit': 'narcissist', 'bigdata': 'big data', '2k17': '2017', '2k18': '2018', 'qouta': 'quota', 'exboyfriend': 'ex boyfriend', 'airhostess': 'air hostess', \"whst\": 'what', 'watsapp': 'whatsapp', 'demonitisation': 'demonetization', 'demonitization': 'demonetization', 'demonetisation': 'demonetization'}\n",
    "\n",
    "def _get_mispell(mispell_dict):\n",
    "    mispell_re = re.compile('(%s)' % '|'.join(mispell_dict.keys()))\n",
    "    return mispell_dict, mispell_re\n",
    "\n",
    "mispellings, mispellings_re = _get_mispell(mispell_dict)\n",
    "def replace_typical_misspell(text):\n",
    "    def replace(match):\n",
    "        return mispellings[match.group(0)]\n",
    "    return mispellings_re.sub(replace, text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "3c9e94579f1850c8c9fc7d0b7d94cdcf4e3a694d"
   },
   "outputs": [],
   "source": [
    "start =time.clock()\n",
    "# lower\n",
    "train_df[\"question_text\"] = train_df[\"question_text\"].apply(lambda x: x.lower())\n",
    "test_df[\"question_text\"] = test_df[\"question_text\"].apply(lambda x: x.lower())\n",
    "\n",
    "# Clean the text\n",
    "train_df[\"question_text\"] = train_df[\"question_text\"].apply(lambda x: clean_text(x))\n",
    "test_df[\"question_text\"] = test_df[\"question_text\"].apply(lambda x: clean_text(x))\n",
    "\n",
    "# Clean numbers\n",
    "train_df[\"question_text\"] = train_df[\"question_text\"].apply(lambda x: clean_numbers(x))\n",
    "test_df[\"question_text\"] = test_df[\"question_text\"].apply(lambda x: clean_numbers(x))\n",
    "\n",
    "# Clean speelings\n",
    "train_df[\"question_text\"] = train_df[\"question_text\"].apply(lambda x: replace_typical_misspell(x))\n",
    "test_df[\"question_text\"] = test_df[\"question_text\"].apply(lambda x: replace_typical_misspell(x))\n",
    "\n",
    "X_train = train_df[\"question_text\"].fillna(\"na\").values  \n",
    "\n",
    "X_test = test_df[\"question_text\"].fillna(\"na\").values\n",
    "y = train_df[\"target\"]\n",
    "end = time.clock()\n",
    "print('Running time: %s Seconds'%(end-start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "4dd860f3d26afb89f1a1521d1ee8663026ce120e"
   },
   "outputs": [],
   "source": [
    "X_train_word_list = []\n",
    "X_test_word_list = []\n",
    "for i in range(len(X_train)):\n",
    "    X_train_word_list.append(X_train[i].split(' '))\n",
    "for i in range(len(X_test)):\n",
    "    X_test_word_list.append(X_test[i].split(' '))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "10aac7db147b6b8814a347cbd98d6c07b2a72ef8"
   },
   "outputs": [],
   "source": [
    "from gensim import corpora\n",
    "# 赋给语料库中每个词(不重复的词)一个整数id\n",
    "word_list = X_train_word_list+X_test_word_list\n",
    "dictionary = corpora.Dictionary(word_list)\n",
    "corpus = [dictionary.doc2bow(text) for text in word_list]\n",
    "# [[(0, 1), (1, 1), (2, 1), (3, 1), (4, 1), (5, 1), (6, 1), (7, 1), (8, 1), (9, 1), (10, 1), (11, 1), (12, 1), (13, 1), (14, 1)]]\n",
    "# 元组中第一个元素是词语在词典对应的id，第二个元素是词语在句子中出现的次数\n",
    "del word_list; gc.collect() \n",
    "\n",
    "# 训练模型并保存\n",
    "from gensim import models\n",
    "tfidf = models.TfidfModel(corpus)\n",
    "# tfidf.save(\"my_model.tfidf\")\n",
    "# # 载入模型\n",
    "# tfidf = models.TfidfModel.load(\"my_model.tfidf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "e4717d4deba0d36194ffde2e43e9cf60aab23169"
   },
   "outputs": [],
   "source": [
    "start = time.clock()\n",
    "X_train_id = []  # 将句子表示成单词在词典中id的形式\n",
    "# [[6, 5, 11, 9, 12, 14, 10, 4, 3, 8, 7, 13, 1, 2, 0], [20, 29, 23, 18, 17, 21, 15, 0, 6, 28, 29, 22, 25, 27, 16, 19, 24, 26, 2, 0], [36, 31, 35, 30, 34, 2, 0, 31, 35, 30, 33, 32, 2, 0]]\n",
    "X_test_id = []\n",
    "word_id_dict = dictionary.token2id\n",
    "for i in range(len(X_train_word_list)):\n",
    "    sen_id = []\n",
    "    word_sen = X_train_word_list[i]\n",
    "    for j in range(len(word_sen)):       \n",
    "        id = word_id_dict.get(word_sen[j])\n",
    "        if id is None:\n",
    "            id = 0 \n",
    "        sen_id.append(id)\n",
    "    X_train_id.append(sen_id)\n",
    "\n",
    "for i in range(len(X_test_word_list)):\n",
    "    sen_id = []\n",
    "    word_sen = X_test_word_list[i]\n",
    "    for j in range(len(word_sen)):       \n",
    "        id = word_id_dict.get(word_sen[j])\n",
    "        if id is None:\n",
    "            id = 0 \n",
    "        sen_id.append(id)\n",
    "    X_test_id.append(sen_id)\n",
    "end = time.clock()\n",
    "print('Running time: %s Minutes'%((end-start)/60))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "48c08eda5778313f377543d64e69e7fe91fdebe8"
   },
   "outputs": [],
   "source": [
    "start = time.clock()\n",
    "X_train_tfidf_vec = []  # 每个句子是一个字典，key是单词的ID，value是单词对应的tfidf值\n",
    "for i in range(len(X_train)):\n",
    "    temp = {}\n",
    "    string = X_train[i]\n",
    "    string_bow = dictionary.doc2bow(string.lower().split()) \n",
    "    string_tfidf = tfidf[string_bow]\n",
    "    # 每个句子是一个list，句中的每个单词表示为一个元组，元组的第一个元素是单词的ID，第二个元素是tfidf值\n",
    "    for j in range(len(string_tfidf)):\n",
    "#         print(string_tfidf[j][0])\n",
    "        temp[string_tfidf[j][0]] = string_tfidf[j][1]\n",
    "#         print(temp)\n",
    "    X_train_tfidf_vec.append(temp)\n",
    "# print(X_train_tfidf_vec)\n",
    "print(len(X_train_tfidf_vec))\n",
    "\n",
    "X_test_tfidf_vec = []  # 每个句子是一个字典，key是单词的ID，value是单词对应的tfidf值\n",
    "for i in range(len(X_test)):\n",
    "    temp = {}\n",
    "    string = X_test[i]\n",
    "    string_bow = dictionary.doc2bow(string.lower().split()) \n",
    "    string_tfidf = tfidf[string_bow]\n",
    "    # 每个句子是一个list，句中的每个单词表示为一个元组，元组的第一个元素是单词的ID，第二个元素是tfidf值\n",
    "    for j in range(len(string_tfidf)):\n",
    "#         print(string_tfidf[j][0])\n",
    "        temp[string_tfidf[j][0]] = string_tfidf[j][1]\n",
    "#         print(temp)\n",
    "    X_test_tfidf_vec.append(temp)\n",
    "print(len(X_test_tfidf_vec))\n",
    "    \n",
    "end = time.clock()\n",
    "print('Running time: %s Seconds'%(end-start))  # 119.35793299999978 Seconds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "2601fd0154cf844e6351a8e50725b39ade2b24d6"
   },
   "outputs": [],
   "source": [
    "X_train_tfidf = []  # tfidf值形成的句子。每个句子是一个list\n",
    "for i in range(len(X_train_id)):\n",
    "    sen_id = X_train_id[i]\n",
    "    sen_id_tfidf = X_train_tfidf_vec[i]\n",
    "    sen = []\n",
    "    for j in range(len(sen_id)):\n",
    "        word_id = sen_id[j]\n",
    "        word_tfidf = sen_id_tfidf.get(word_id)\n",
    "        if word_tfidf is None:\n",
    "            word_tfidf = 0\n",
    "        sen.append(word_tfidf)\n",
    "    X_train_tfidf.append(sen)\n",
    "\n",
    "X_test_tfidf = []  # tfidf值形成的句子。每个句子是一个list\n",
    "for i in range(len(X_test_id)):\n",
    "    sen_id = X_test_id[i]\n",
    "    sen_id_tfidf = X_test_tfidf_vec[i]\n",
    "    sen = []\n",
    "    for j in range(len(sen_id)):\n",
    "        word_id = sen_id[j]\n",
    "        word_tfidf = sen_id_tfidf.get(word_id)\n",
    "        if word_tfidf is None:\n",
    "            word_tfidf = 0\n",
    "        sen.append(word_tfidf)\n",
    "    X_test_tfidf.append(sen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "708eb13259feed675013d35c084c234cc6a673d4"
   },
   "outputs": [],
   "source": [
    "x_train_tfidf = sequence.pad_sequences(X_train_tfidf, maxlen=maxlen,dtype='float64') # (1306122, 70)\n",
    "x_test_tfidf = sequence.pad_sequences(X_test_tfidf, maxlen=maxlen,dtype='float64') # (56370, 70)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "5cd7bf6d6729516b1fa6bd4f4449ed16d192f1ea"
   },
   "outputs": [],
   "source": [
    "del X_train_word_list; gc.collect() \n",
    "del X_test_word_list; gc.collect() \n",
    "del X_train_id; gc.collect() \n",
    "del X_test_id; gc.collect() \n",
    "del X_train_tfidf_vec; gc.collect() \n",
    "del X_test_tfidf_vec; gc.collect() \n",
    "del X_train_tfidf; gc.collect() \n",
    "del X_test_tfidf; gc.collect() \n",
    "del tfidf; gc.collect() \n",
    "del corpus; gc.collect() \n",
    "del dictionary; gc.collect() \n",
    "del word_id_dict; gc.collect() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "739505d6a2dfbc2d83e054ba9fa07125a19c8a0d"
   },
   "outputs": [],
   "source": [
    "tokenizer = text.Tokenizer(num_words=max_features)\n",
    "tokenizer.fit_on_texts(list(X_train) + list(X_test))\n",
    "\n",
    "X_train_emb = tokenizer.texts_to_sequences(X_train)\n",
    "X_test_emb = tokenizer.texts_to_sequences(X_test)\n",
    "\n",
    "x_train_emb = sequence.pad_sequences(X_train_emb, maxlen=maxlen) # (1306122, 70)\n",
    "x_test_emb = sequence.pad_sequences(X_test_emb, maxlen=maxlen) # (56370, 70)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "a7bb2dde23b271fe1fa3e28bc0954123c29a74d3"
   },
   "outputs": [],
   "source": [
    "del X_train_emb; gc.collect() \n",
    "del X_test_emb; gc.collect() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "0700ae2e6517f94c4bc894a8f9c652dab2a979bd"
   },
   "outputs": [],
   "source": [
    "print(x_train_emb.shape)\n",
    "print(x_test_emb.shape)\n",
    "print(x_train_emb.astype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "da9657e968fdbc5aea8af8c3d1b2fa7482bfa379"
   },
   "outputs": [],
   "source": [
    "x_train_input = np.concatenate((x_train_emb, x_train_tfidf), axis=1)\n",
    "x_test_input = np.concatenate((x_test_emb, x_test_tfidf), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "0f98849546e663c676d58937c27b6bef47271d8f"
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_tra, X_val, y_tra, y_val = train_test_split(x_train_input, y, test_size = 0.1, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "1c711eb912939e4d90c8e38210cc5b26ca0edd85"
   },
   "outputs": [],
   "source": [
    "print(x_test_input[0][139:140])\n",
    "print(x_train_emb[0][68:70])\n",
    "print(x_train_tfidf[0][0:2])\n",
    "print(x_train_input.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "91d49cc0faab840a87c2e39cceb4c260c97bbefb"
   },
   "outputs": [],
   "source": [
    "del x_train_emb; gc.collect() \n",
    "del x_train_tfidf; gc.collect() \n",
    "del x_test_emb; gc.collect() \n",
    "del x_test_tfidf; gc.collect() \n",
    "del x_train_input; gc.collect() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "ee3d59741f871ac0f6d87f5bb30e036722dc1a1a"
   },
   "outputs": [],
   "source": [
    "def attention_3d_block(inputs):\n",
    "    # inputs.shape = (batch_size, time_steps, input_dim)\n",
    "    TIME_STEPS = inputs.shape[1].value\n",
    "    SINGLE_ATTENTION_VECTOR = False\n",
    "    \n",
    "    input_dim = int(inputs.shape[2])\n",
    "    a = Permute((2, 1))(inputs)\n",
    "    a = Reshape((input_dim, TIME_STEPS))(a) # this line is not useful. It's just to know which dimension is what.\n",
    "    a = Dense(TIME_STEPS, activation='softmax')(a)\n",
    "    if SINGLE_ATTENTION_VECTOR:\n",
    "        a = Lambda(lambda x: K.mean(x, axis=1))(a)\n",
    "        a = RepeatVector(input_dim)(a)\n",
    "    a_probs = Permute((2, 1))(a)\n",
    "    output_attention_mul = Multiply()([inputs, a_probs])\n",
    "    return output_attention_mul"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "a81f007cb316c5b7a87d9aa7cdec56ed4fcdca30"
   },
   "outputs": [],
   "source": [
    "from keras import backend as K\n",
    "from keras.engine.topology import Layer, InputSpec\n",
    "from keras import initializers\n",
    "\n",
    "class AttLayer(Layer):\n",
    "    def __init__(self, attention_dim):\n",
    "        self.init = initializers.get('normal')\n",
    "        self.supports_masking = True\n",
    "        self.attention_dim = attention_dim\n",
    "        super(AttLayer, self).__init__()\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        assert len(input_shape) == 3\n",
    "        self.W = K.variable(self.init((input_shape[-1], self.attention_dim)))\n",
    "        self.b = K.variable(self.init((self.attention_dim, )))\n",
    "        self.u = K.variable(self.init((self.attention_dim, 1)))\n",
    "        self.trainable_weights = [self.W, self.b, self.u]\n",
    "        super(AttLayer, self).build(input_shape)\n",
    "\n",
    "    def compute_mask(self, inputs, mask=None):\n",
    "        return mask\n",
    "\n",
    "    def call(self, x, mask=None):\n",
    "        # size of x :[batch_size, sel_len, attention_dim]\n",
    "        # size of u :[batch_size, attention_dim]\n",
    "        # uit = tanh(xW+b)\n",
    "        uit = K.tanh(K.bias_add(K.dot(x, self.W), self.b))\n",
    "        ait = K.dot(uit, self.u)\n",
    "        ait = K.squeeze(ait, -1)\n",
    "\n",
    "        ait = K.exp(ait)\n",
    "\n",
    "        if mask is not None:\n",
    "            # Cast the mask to floatX to avoid float64 upcasting in theano\n",
    "            ait *= K.cast(mask, K.floatx())\n",
    "        ait /= K.cast(K.sum(ait, axis=1, keepdims=True) + K.epsilon(), K.floatx())\n",
    "        ait = K.expand_dims(ait)\n",
    "        weighted_input = x * ait\n",
    "        output = K.sum(weighted_input, axis=1)\n",
    "\n",
    "        return output\n",
    "\n",
    "    def compute_output_shape(self, input_shape):\n",
    "        return (input_shape[0], input_shape[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "a4c2023574cdc43b4b26952bae8c766a84e4a111"
   },
   "outputs": [],
   "source": [
    "# EMBEDDING_FILE = '/Users/zhaofangyu/Downloads/glove.6B/glove.6B.300d.txt'\n",
    "# def get_coefs(word,*arr): return word, np.asarray(arr, dtype='float32')\n",
    "# embeddings_index = dict(get_coefs(*o.split(\" \")) for o in open(EMBEDDING_FILE))\n",
    "# print(\"embedding_1 !!\")\n",
    "# all_embs = np.stack(embeddings_index.values())\n",
    "# emb_mean,emb_std = all_embs.mean(), all_embs.std()\n",
    "# embed_size = all_embs.shape[1]\n",
    "\n",
    "# word_index = tokenizer.word_index\n",
    "# nb_words = min(max_features, len(word_index))\n",
    "# embedding_matrix_1 = np.random.normal(emb_mean, emb_std, (nb_words, embed_size))\n",
    "# for word, i in word_index.items():\n",
    "#     if i >= max_features: continue\n",
    "#     embedding_vector = embeddings_index.get(word)\n",
    "#     if embedding_vector is not None: embedding_matrix_1[i] = embedding_vector\n",
    "\n",
    "# del embeddings_index; gc.collect() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "65a5fed11205f1d614b1862e163878e32fe21591"
   },
   "outputs": [],
   "source": [
    "# EMBEDDING_FILE = '../input/embeddings/paragram_300_sl999/paragram_300_sl999.txt'\n",
    "# def get_coefs(word,*arr): return word, np.asarray(arr, dtype='float32')\n",
    "# embeddings_index = dict(get_coefs(*o.split(\" \")) for o in open(EMBEDDING_FILE, encoding=\"utf8\", errors='ignore') if len(o)>100)\n",
    "# print(\"embedding_3 !!\")\n",
    "# all_embs = np.stack(embeddings_index.values())\n",
    "# emb_mean,emb_std = all_embs.mean(), all_embs.std()\n",
    "# embed_size = all_embs.shape[1]\n",
    "\n",
    "# word_index = tokenizer.word_index\n",
    "# nb_words = min(max_features, len(word_index))\n",
    "# embedding_matrix_3 = np.random.normal(emb_mean, emb_std, (nb_words, embed_size))\n",
    "# for word, i in word_index.items():\n",
    "#     if i >= max_features: continue\n",
    "#     embedding_vector = embeddings_index.get(word)\n",
    "#     if embedding_vector is not None: embedding_matrix_3[i] = embedding_vector\n",
    "        \n",
    "# del embeddings_index; gc.collect()   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "348676861554b963aca115d079ebc103395ee229"
   },
   "outputs": [],
   "source": [
    "# embedding_matrix = np.mean((embedding_matrix_1), axis=0)# (90000, 300)\n",
    "# print(embedding_matrix.shape)\n",
    "# del embedding_matrix_1; gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "c6041e9d13cbf27804ecced3465dbce9d8084898"
   },
   "outputs": [],
   "source": [
    "def row_slice(x, start, end):\n",
    "    return x[:, start:end]\n",
    "def change_type(x):\n",
    "    return tf.to_int32(x, name='ToInt32')\n",
    "def matmul(x, y):\n",
    "    return tf.matmul(x, y)\n",
    "def concat(x, y, axis):\n",
    "    return tf.concat([x, y], axis=axis)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model2():\n",
    "    inp = Input(shape=(maxlen*2, ))\n",
    "    embed = Embedding(max_features, embed_size * 1, trainable=False)(inp)\n",
    "    x = embed\n",
    "    x = Bidirectional(LSTM(128, unroll=True,return_sequences=True))(x)\n",
    "    x = attention_3d_block(x)\n",
    "    x = Bidirectional(LSTM(128, unroll=True,return_sequences=True))(x)\n",
    "    x = AttLayer(64)(x)\n",
    "    x = Dropout(0.3)(x)\n",
    "    x = Dense(128, activation='relu')(x)\n",
    "    outp = Dense(1, activation=\"sigmoid\")(x)\n",
    "    model = Model(inputs=inp, outputs=outp)\n",
    "    model.compile(loss='binary_crossentropy',\n",
    "                  optimizer='adam',\n",
    "                  metrics=['accuracy']) \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "a6fcecbb832d3f01fcd6de79fdcb87354e3199d5"
   },
   "outputs": [],
   "source": [
    "def model2():\n",
    "    inp = Input(shape=(maxlen*2, ))  # (?, 140)\n",
    "    print(\"inp\",inp.shape)\n",
    "    inp_emb = Lambda(row_slice, arguments={'start':0, 'end':70})(inp)  # (?, 70)  <dtype: 'float32'>\n",
    "    inp_emb = Lambda(change_type)(inp_emb)  # tensor <dtype: 'int32'>\n",
    "\n",
    "    inp_tfidf = Lambda(row_slice, arguments={'start':70, 'end':140})(inp)  # # (?, 70)\n",
    "    inp_tfidf = Reshape((maxlen, 1))(inp_tfidf)  # tensor (?, 70, 1)\n",
    "\n",
    "    \n",
    "#     embed = Embedding(max_features, embed_size * 1, weights=[embedding_matrix], trainable=False)(inp_emb)\n",
    "    embed = Embedding(max_features, embed_size * 1, trainable=False)(inp_emb)\n",
    "    # (?, 70, 300)\n",
    "    embed_trans = Permute((2, 1))(embed)  # (?, 300, 70)\n",
    "    # arguments={'x':embed_trans, 'y':inp_tfidf}\n",
    "    sen_emb = Lambda(matmul, arguments={'y':inp_tfidf})(embed_trans)   # (?, 300, 1)\n",
    "    emb_mul_tfidf = Lambda(matmul, arguments={'y': Permute((2, 1))(sen_emb)})(inp_tfidf)   # (?, 70, 300)\n",
    "    \n",
    "    x = Lambda(concat, arguments={'y':emb_mul_tfidf, 'axis':2})(embed)\n",
    "#     x = tf.concat([embed, emb_mul_tfidf], axis=2)  # (?, 70, 600)\n",
    "    print(x.shape)\n",
    "    \n",
    "    x = Bidirectional(LSTM(128, return_sequences=True))(x)\n",
    "    print(\"x shape: \", x.shape)\n",
    "    x = BatchNormalization()(x)\n",
    "    print(\"x shape: \", x.shape)  # (?, 70, 256)\n",
    "    x = attention_3d_block(x)\n",
    "    print(\"x shape: \", x.shape)\n",
    "    x = BatchNormalization()(x)\n",
    "    print(\"x shape after attention: \", x.shape)  # (?, 70, 256)\n",
    "    x = Bidirectional(LSTM(128, return_sequences=True))(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    print(\"x shape: \", x.shape)  # (?, 70, 256)\n",
    "    x = AttLayer(64)(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    print(\"x shape after attention: \", x.shape)  # (?, 256)\n",
    "    x = Dropout(0.3)(x)\n",
    "    x = Dense(128, activation='relu')(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    outp = Dense(1, activation=\"sigmoid\")(x)\n",
    "    outp = Dense(1, activation=\"sigmoid\")(x)\n",
    "    print(\"outp:\",outp.shape)\n",
    "    model = Model(inputs=inp, outputs=outp)\n",
    "    model.compile(loss='binary_crossentropy',\n",
    "                  optimizer='adam',\n",
    "                  metrics=['accuracy'])    \n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "794cb9fca1b614c1759efab3450defc8cfe06bc2"
   },
   "outputs": [],
   "source": [
    "# print(X_tra.shape)\n",
    "MODEL2 = model2()\n",
    "MODEL2.summary()\n",
    "\n",
    "batch_size = 512\n",
    "epochs = 4\n",
    "\n",
    "early_stopping = EarlyStopping(patience=3, verbose=1, monitor='val_loss', mode='min')\n",
    "model_checkpoint = ModelCheckpoint('./model2.model', save_best_only=True, verbose=1, monitor='val_loss', mode='min')\n",
    "reduce_lr = ReduceLROnPlateau(factor=0.5, patience=3, min_lr=0.0001, verbose=1)\n",
    "\n",
    "hist = MODEL2.fit(X_tra, y_tra, batch_size=batch_size, epochs=epochs, validation_data=(X_val, y_val), verbose=True)\n",
    "# MODEL2.save('./model2.h5')\n",
    "print(\"finished!!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "07d16b8ac5b6b6df6aa3477a542b83b570297b01"
   },
   "outputs": [],
   "source": [
    "pred_val_y_2 = MODEL2.predict([X_val], batch_size=1024, verbose=1)\n",
    "thresholds = []\n",
    "for thresh in np.arange(0.1, 0.501, 0.01):\n",
    "    thresh = np.round(thresh, 2)\n",
    "    acc = metrics.accuracy_score(y_val, (pred_val_y_2 > thresh).astype(int))\n",
    "    f1_score = metrics.f1_score(y_val, (pred_val_y_2 > thresh).astype(int))\n",
    "    pre = metrics.precision_score(y_val, (pred_val_y_2 > thresh).astype(int))\n",
    "    recall = metrics.recall_score(y_val, (pred_val_y_2 > thresh).astype(int))\n",
    "    thresholds.append([thresh, f1_score])\n",
    "    print(\"acc score at threshold {0} is {1}\".format(thresh, acc))\n",
    "    print(\"F1 score at threshold {0} is {1}\".format(thresh, f1_score))\n",
    "    print(\"precision at threshold {0} is {1}\".format(thresh, pre))\n",
    "    print(\"recall at threshold {0} is {1}\".format(thresh, recall))\n",
    "    \n",
    "thresholds.sort(key=lambda x: x[1], reverse=True)\n",
    "best_thresh = thresholds[0][0]\n",
    "best_thresh_f1 = thresholds[0][1]\n",
    "print(\"Best threshold:{0} ， f1:{1}\".format(str(best_thresh), str(best_thresh_f1)))\n",
    "# print(\"Best threshold:{0} ， f1:{1}\".format(best_thresh_2, best_thresh_2_f1))\n",
    "\n",
    "# metrics.roc_curve(pred_val_y_2, y_val, pos_label=None, sample_weight=None, drop_intermediate=True)\n",
    "\n",
    "y_pred_2 = MODEL2.predict(x_test_input, batch_size=1024, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "71275c898ad2bddaf6632e69820bf8ddf5c31ead"
   },
   "outputs": [],
   "source": [
    "y_pred = y_pred_2\n",
    "y_te = (y_pred[:,0] > best_thresh).astype(np.int)\n",
    "\n",
    "submit_df = pd.DataFrame({\"qid\": test_df[\"qid\"], \"prediction\": y_te})\n",
    "submit_df.to_csv(\"submission.csv\", index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
